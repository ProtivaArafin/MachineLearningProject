# -*- coding: utf-8 -*-
"""FINAL PROJECT DELIVERABLE 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DmGpfpS5yPyvomrO8NpnGW-ydvmZ3r8y

Dataset Link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data

Input:

Predict: benign/magllignant
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import kagglehub
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp
from sklearn import preprocessing
import matplotlib.ticker as mticker
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import random
import os

# Load dataset
dataset_path = kagglehub.dataset_download("uciml/breast-cancer-wisconsin-data")
data = pd.read_csv(f"{dataset_path}/data.csv").drop(['id', 'Unnamed: 32'], axis=1)

data.head()

data.describe()

data.info()

data.isnull().sum()

data.nunique()

# Map diagnosis column to binary values
data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})
y=data['diagnosis']

#PCA
# Standardize
features = data.drop('diagnosis', axis=1)
features_standardized = (features - features.mean()) / features.std()
features_standardized = features_standardized.fillna(0)
# Apply PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(features_standardized)

# Histograms
plt.figure(figsize=(20, 15))
# First 12 features
for i, feature in enumerate(features_standardized.columns[:12]):
    plt.subplot(4, 3, i + 1)
    sns.histplot(features_standardized[feature], kde=True, color='skyblue', edgecolor='black')
    plt.title(feature, fontsize=12)
    plt.xlabel('')
plt.suptitle("Histograms of Features (Standardized)", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# Pair Plot for Key Features
selected_features = ['radius_mean', 'texture_mean', 'area_mean', 'perimeter_mean', 'diagnosis']
sns.pairplot(data=data[selected_features], hue='diagnosis', palette='coolwarm', diag_kind='kde', markers=["o", "s"])
plt.suptitle("Pair Plot of Selected Features", y=1.02, fontsize=16)
plt.show()

#Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

data_train, data_test, target_train, target_test = train_test_split(principal_components, y, test_size=0.2, random_state=42)

forest_model = RandomForestClassifier(
    n_estimators=120,
    max_leaf_nodes=15,
    random_state=123)
# Training
forest_model.fit(data_train, target_train)

# Predicting
predicted_labels = forest_model.predict(data_test)

# Computing accuracy
accuracy_val = accuracy_score(target_test, predicted_labels)
print(f"Accuracy: {accuracy_val:.3f}")

# Showing more detail
print("\nPerformance breakdown:")
report_details = classification_report(target_test, predicted_labels)
print(report_details)

#SVM

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score

X = principal_components
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm_bd=SVC(kernel='linear', C=1,random_state=0)
svm_bd.fit(X_train, y_train)

predictions_svm = svm_bd.predict(X_test)

accuracy_svm = accuracy_score(y_test, predictions_svm)
print(f"Overall accuracy: {accuracy_svm:.3f}")

svm_report = classification_report(y_test, predictions_svm)
print("\nPerformance breakdown:")
print(svm_report)

f1_score_svm = f1_score(y_test, predictions_svm)
print(f"F1 Score: {f1_score_svm:.3f}")

f1_score_svm_0 = f1_score(y_test, predictions_svm,pos_label=0)
print(f"F1 Score for 0 class: {f1_score_svm_0:.3f}")

f1_score_svm_1 = f1_score(y_test, predictions_svm,pos_label=1)
print(f"F1 Score for 1 class: {f1_score_svm_1:.3f}")

#XGBoost

G_Train,G_Test,b_Train,b_Test=train_test_split(principal_components,y,test_size=0.2,random_state=42)
import xgboost
from xgboost import XGBClassifier

xgboost_bd=xgboost.XGBClassifier(eval_metric='logloss',use_label_encoder=False,n_estimators=50,max_depth=50)
xgboost_bd.fit(G_Train,b_Train)

xgbosst_predictions=xgboost_bd.predict(G_Test)

accuracy_xgboost = accuracy_score(b_Test, xgbosst_predictions)
print(f"Overall accuracy: {accuracy_xgboost:.3f}")

xgboost_report = classification_report(b_Test, xgbosst_predictions)
print("\nPerformance breakdown:")
print(xgboost_report)

# K-Nearest Neighbors (KNN)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(principal_components, y, test_size=0.2, random_state=42)

# Define the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)

# Training
knn_model.fit(X_train, y_train)

# Predicting
knn_predictions = knn_model.predict(X_test)

# Computing accuracy
accuracy_knn = accuracy_score(y_test, knn_predictions)
print(f"Overall accuracy: {accuracy_knn:.3f}")

# Showing more detail
knn_report = classification_report(y_test, knn_predictions)
print("\nPerformance breakdown:")
print(knn_report)

# Naive Bayes
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(principal_components, y, test_size=0.2, random_state=42)


# Define the Naive Bayes model
nb_model = GaussianNB()

# Training the model
nb_model.fit(X_train, y_train)

# Predicting on the test set
nb_predictions = nb_model.predict(X_test)

# Computing accuracy
accuracy_nb = accuracy_score(y_test, nb_predictions)
print(f"Overall accuracy: {accuracy_nb:.3f}")

# Showing more detail
nb_report = classification_report(y_test, nb_predictions)
print("\nPerformance breakdown:")
print(nb_report)

# Comparison Table
model_accuracies = {
    "Model": ["Random Forest", "SVM", "XGBoost", "KNN", "Naive Bayes"],
    "Accuracy": [accuracy_val, accuracy_svm, accuracy_xgboost, accuracy_knn, accuracy_nb]
}

# Creating a DataFrame
accuracy_df = pd.DataFrame(model_accuracies, index=range(1, len(model_accuracies["Model"]) + 1))

# Display the summary table
print("Comparison Table:")
print(accuracy_df)

colors=['red','blue','green','maroon','violet']

ax=accuracy_df.plot(x='Model',y='Accuracy',kind='bar',color=colors,figsize=(6,6),legend=False)
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison of each model')
plt.show

#CROSS VALIDATION

classfiers = {
    "Random Forest": RandomForestClassifier(n_estimators=120, max_leaf_nodes=15, random_state=123),
    "SVM": SVC(kernel='linear', C=1, random_state=0),
    "XGBoost": XGBClassifier(eval_metric='logloss', use_label_encoder=False, n_estimators=50, max_depth=50),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes": GaussianNB()
}

k=10
results = {}

for name, clf in classfiers.items():
    scores = cross_val_score(clf, principal_components, y, cv=k, scoring='accuracy')
    results[name] = {
        "Mean Accuracy": np.mean(scores),
        "Standard Deviation": np.std(scores),
        "scores":scores
    }
for name, result in results.items():
    print(f"{name}:")
    print(f"Mean Accuracy: {result['Mean Accuracy']:.3f}")
    print(f"Standard Deviation: {result['Standard Deviation']:.3f}")

results_df = pd.DataFrame([
    {
        "Classifier": name,
        "Mean Accuracy": metrics["Mean Accuracy"],
        "Standard Deviation": metrics["Standard Deviation"],
        **{f"Fold {i+1}": score for i, score in enumerate(metrics["scores"])}
    }
    for name, metrics in results.items()
])

# Plot Mean Accuracy
plt.figure(figsize=(10, 6))
sns.barplot(
    x=results_df["Classifier"],
    y=results_df["Mean Accuracy"],
    palette="viridis"
)
plt.title("Mean Accuracy of Classifiers")
plt.ylabel("Mean Accuracy")
plt.xlabel("Classifier")
plt.xticks(rotation=45)
plt.show()

# Display Table
print("Cross-Validation Results:")
print(results_df.to_string(index=False))